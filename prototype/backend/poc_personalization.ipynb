{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# POC: Bayesian contrastive vs prompt baseline\n",
        "\n",
        "This notebook runs the minimal POC on a JSONL edit-trace file and reports held-out preference accuracy. Update `DATA_PATH` and `MODEL_NAME` to match your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/scratch/yirenl2/projects/context-steering-writing/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\n"
          ]
        }
      ],
      "source": [
        "from poc.data import load_jsonl, split_by_user, group_by_user\n",
        "from poc.contexts import DEFAULT_CONTEXTS, contexts_as_strings\n",
        "from poc.method_a import BayesContrastiveModel\n",
        "from poc.prompt_baseline import build_style_profile\n",
        "from poc.eval import evaluate_method_a, evaluate_prompt_baseline\n",
        "from cos.utils import load_hf_model_and_tokenizer\n",
        "import torch\n",
        "\n",
        "DATA_PATH = \"./data/edits.jsonl\"\n",
        "MODEL_NAME = \"llama-2-7b-chat\"\n",
        "\n",
        "examples = load_jsonl(DATA_PATH)\n",
        "train, val = split_by_user(examples, val_fraction=0.2)\n",
        "train_by_user = group_by_user(train)\n",
        "\n",
        "model, tokenizer = load_hf_model_and_tokenizer(MODEL_NAME)\n",
        "contexts = contexts_as_strings(DEFAULT_CONTEXTS)\n",
        "\n",
        "prior_mean = torch.zeros(len(contexts), device=model.device)\n",
        "prior_cov = torch.eye(len(contexts), device=model.device) * 4.0\n",
        "\n",
        "bayes_model = BayesContrastiveModel(\n",
        "    contexts=contexts,\n",
        "    prior_mean=prior_mean,\n",
        "    prior_cov=prior_cov,\n",
        "    beta=1.0,\n",
        "    is_chat=True,\n",
        ")\n",
        "\n",
        "for ex in train:\n",
        "    bayes_model.update_user(model, tokenizer, ex)\n",
        "\n",
        "profiles = {u: build_style_profile(u, edits) for u, edits in train_by_user.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bayesian contrastive: {'accuracy': 0.5, 'auc': 0.625, 'mean_delta': 2.9626617431640625}\n",
            "Prompt baseline: {'accuracy': 0.5, 'auc': 0.5, 'mean_delta': 2.293132781982422}\n"
          ]
        }
      ],
      "source": [
        "metrics_a = evaluate_method_a(model, tokenizer, val, bayes_model)\n",
        "metrics_b = evaluate_prompt_baseline(model, tokenizer, val, profiles, is_chat=True)\n",
        "\n",
        "print(\"Bayesian contrastive:\", metrics_a)\n",
        "print(\"Prompt baseline:\", metrics_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "Prompt: Draft a short update about the project status.\n",
            "Baseline: Sure, here's an update about the project status in the style profile you provided:\n",
            "\n",
            "Project Status Update:\n",
            "\n",
            "The project is progressing well, with [0.70] of the work completed so far. Our team has been working diligently to ensure the project's success, and we are on track to meet our deadline. Currently, we are focused on [10.0 words] and are making good progress. Overall, we are satisfied with the project's progress and are confident in our ability to deliver a high-quality outcome.\n",
            "\n",
            "I hope this update provides a\n",
            "CoS:  Project Update:\n",
            "\n",
            "Progress steady on schedule, as planned. Key milestones met, with additional achievements forthcoming. Brief update on notable developments:\n",
            "\n",
            "* Completed coding phase with 95% accuracy rate, exceeding benchmark.\n",
            "* Conducted thorough testing, identified and addressed minor bugs.\n",
            "* Design phase advancing as planned, with 75% design elements finalized.\n",
            "* Collaboration with cross-functional teams proceeding smoothly.\n",
            "\n",
            "In summary, project progressing according to plan, with notable achievements in coding, testing, and design. Rem\n",
            "---\n",
            "Prompt: Summarize the meeting outcome in 1–2 sentences.\n",
            "Baseline: Sure, here's my attempt at summarizing the meeting outcome in the requested writing style:\n",
            "\n",
            "The meeting outcome was largely successful, with key decisions made and action items assigned to various team members. Notable outcomes included the approval of a new marketing strategy and the allocation of additional resources for product development.\n",
            "CoS:  Project Update:\n",
            "\n",
            "The project is making steady progress, marked by tangible milestones and a clear trajectory towards completion. The development team has been diligently working on the software, incorporating customer feedback and addressing any technical issues that arise. The user interface has been refined, resulting in an intuitive and user-friendly experience. The testing phase is underway, with a focus on ensuring the software's robustness and reliability. The project manager is closely monitoring the progress, maintaining open lines of communication with the development team and stakeholders. The project remains on track,\n"
          ]
        }
      ],
      "source": [
        "from cos.core import multi_contextual_steering_hf\n",
        "from poc.contexts import lambdas_from_slider\n",
        "\n",
        "# Demo: generate side-by-side outputs for one user\n",
        "DEMO_PROMPTS = [\n",
        "    \"Draft a short update about the project status.\",\n",
        "    \"Summarize the meeting outcome in 1–2 sentences.\",\n",
        "]\n",
        "DEMO_USER = next(iter(train_by_user.keys()))\n",
        "SLIDER_VALUE = 6\n",
        "\n",
        "# Prompt baseline generation\n",
        "style_prompt = profiles[DEMO_USER].prompt_text\n",
        "baseline_outputs = []\n",
        "for prompt in DEMO_PROMPTS:\n",
        "    full_prompt = f\"{style_prompt}\\n\\n{prompt}\" if prompt else style_prompt\n",
        "    dialog = [{\"role\": \"user\", \"content\": full_prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        dialog,\n",
        "        tokenize=True,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=False,\n",
        "        return_dict=True,\n",
        "    ).to(model.device)\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    prompt_len = inputs.input_ids.shape[1]\n",
        "    gen_text = tokenizer.decode(output_ids[0, prompt_len:], skip_special_tokens=True)\n",
        "    baseline_outputs.append(gen_text.strip())\n",
        "\n",
        "# CoS generation using learned lambdas\n",
        "mean = bayes_model.user_models[DEMO_USER].posterior.mean.tolist()\n",
        "contexts = contexts_as_strings(DEFAULT_CONTEXTS)\n",
        "cos_lambdas = lambdas_from_slider(SLIDER_VALUE, mean)\n",
        "all_contexts = [[c for _ in DEMO_PROMPTS] for c in contexts]\n",
        "all_lambdas = [[l for _ in DEMO_PROMPTS] for l in cos_lambdas]\n",
        "\n",
        "cos_output = multi_contextual_steering_hf(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    prompts=DEMO_PROMPTS,\n",
        "    all_contexts=all_contexts,\n",
        "    all_lambdas=all_lambdas,\n",
        "    is_chat=True,\n",
        "    show_progress=False,\n",
        "    max_gen_len=128,\n",
        ")\n",
        "\n",
        "for prompt, base, cos in zip(DEMO_PROMPTS, baseline_outputs, cos_output[\"generation\"]):\n",
        "    print(\"---\")\n",
        "    print(\"Prompt:\", prompt)\n",
        "    print(\"Baseline:\", base)\n",
        "    print(\"CoS:\", cos[\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote 14 rows to /scratch/yirenl2/projects/context-steering-writing/outputs/slider_sweep.jsonl\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "OUTPUT_PATH = Path(\"/scratch/yirenl2/projects/context-steering-writing/outputs/slider_sweep.jsonl\")\n",
        "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "slider_values = list(range(1, 8))\n",
        "rows = []\n",
        "\n",
        "for slider in slider_values:\n",
        "    # Baseline outputs\n",
        "    baseline_outputs = []\n",
        "    for prompt in DEMO_PROMPTS:\n",
        "        full_prompt = f\"{style_prompt}\\n\\n{prompt}\" if prompt else style_prompt\n",
        "        dialog = [{\"role\": \"user\", \"content\": full_prompt}]\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            dialog,\n",
        "            tokenize=True,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=False,\n",
        "            return_dict=True,\n",
        "        ).to(model.device)\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "        prompt_len = inputs.input_ids.shape[1]\n",
        "        gen_text = tokenizer.decode(output_ids[0, prompt_len:], skip_special_tokens=True)\n",
        "        baseline_outputs.append(gen_text.strip())\n",
        "\n",
        "    # CoS outputs\n",
        "    cos_lambdas = lambdas_from_slider(slider, mean)\n",
        "    all_contexts = [[c for _ in DEMO_PROMPTS] for c in contexts]\n",
        "    all_lambdas = [[l for _ in DEMO_PROMPTS] for l in cos_lambdas]\n",
        "    cos_output = multi_contextual_steering_hf(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompts=DEMO_PROMPTS,\n",
        "        all_contexts=all_contexts,\n",
        "        all_lambdas=all_lambdas,\n",
        "        is_chat=True,\n",
        "        show_progress=False,\n",
        "        max_gen_len=128,\n",
        "    )\n",
        "\n",
        "    for prompt, base, cos in zip(DEMO_PROMPTS, baseline_outputs, cos_output[\"generation\"]):\n",
        "        rows.append({\n",
        "            \"user_id\": DEMO_USER,\n",
        "            \"slider\": slider,\n",
        "            \"prompt\": prompt,\n",
        "            \"baseline\": base,\n",
        "            \"cos\": cos[\"content\"],\n",
        "        })\n",
        "\n",
        "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for row in rows:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Wrote {len(rows)} rows to {OUTPUT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
